{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd00acd7",
   "metadata": {},
   "source": [
    "# cifar10 vgg16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b5296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "torch.__version__\n",
    "\n",
    "import os\n",
    "import sys \n",
    "import argparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13ef56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f97a418c970>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e3c8b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'{device}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2976df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 224\n",
    "classes = ('0_Rust', '1_Brown_Spots', '2_Sooty_Molds')\n",
    "stage = 'stage_2'\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "668807fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Resize(size=[size, size]),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(size=[size, size]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a78dda4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in train dataset: 6000\n",
      "Number of images in test/validation dataset: 625\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "\n",
    "        classes = os.listdir(self.root)\n",
    "        for class_name in classes:\n",
    "            class_path = os.path.join(self.root, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                images = os.listdir(class_path)\n",
    "                for image_name in images:\n",
    "                    image_path = os.path.join(class_path, image_name)\n",
    "                    self.image_paths.append((image_path, int(class_name.split('_')[0])))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Create custom datasets\n",
    "dataset_folder = r'./dataset/'\n",
    "\n",
    "train_folder = os.path.join(dataset_folder, f'swatdcnn/data/Augmented/{stage}/train')\n",
    "test_folder = os.path.join(dataset_folder, f'swatdcnn/data/Augmented/{stage}/validation')\n",
    "\n",
    "train_dataset = CustomDataset(root=train_folder, transform=transform_train)\n",
    "test_dataset = CustomDataset(root=test_folder, transform=transform_test)\n",
    "\n",
    "train_dataset_size = len(train_dataset)\n",
    "test_dataset_size = len(test_dataset)\n",
    "\n",
    "print(f\"Number of images in train dataset: {train_dataset_size}\")\n",
    "print(f\"Number of images in test/validation dataset: {test_dataset_size}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c301ba2b",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e352fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c44b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookat_dataset(dataset, istensor=False):\n",
    "  figure = plt.figure(figsize=(8, 8))\n",
    "  rows, cols = 2, 2\n",
    "  for i in range(1, 5):\n",
    "      sample_idx = torch.randint(len(dataset), size=(1,)).item()\n",
    "      img, label = dataset[sample_idx]\n",
    "      figure.add_subplot(rows, cols, i)\n",
    "      plt.title(CATEGORIES[label])\n",
    "      plt.axis(\"off\")\n",
    "      if istensor:\n",
    "        plt.imshow(img.squeeze().permute(1, 2, 0))\n",
    "      else:\n",
    "        plt.imshow(img)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2a41967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookat_dataset(trainloader, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0546f7",
   "metadata": {},
   "source": [
    "# Creating the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a1c73e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vit_b_16 = models.vit_b_16(weights='DEFAULT')\n",
    "net = models.vgg16(weights='DEFAULT')\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac969755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing the network\n",
    "for param in vit_b_16.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a041d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the last fully connected layer\n",
    "num_features = vit_b_16.heads.head.in_features\n",
    "print('num_features: ', num_features)\n",
    "\n",
    "vit_b_16.conv_proj = nn.Conv2d(3, 768, kernel_size=(4, 4), stride=(4, 4))\n",
    "\n",
    "vit_b_16.encoder.layers = nn.Sequential(\n",
    "    vit_b_16.encoder.layers[0],  # Keep encoder_layer_0\n",
    "    vit_b_16.encoder.layers[1],  # Keep encoder_layer_1\n",
    "    vit_b_16.encoder.layers[2],  # Keep encoder_layer_2\n",
    "    vit_b_16.encoder.layers[3],  # Keep encoder_layer_3\n",
    "    # vit_b_16.encoder.layers[4],  # Keep encoder_layer_4\n",
    "    # vit_b_16.encoder.layers[5],  # Keep encoder_layer_5\n",
    "    # vit_b_16.encoder.layers[6],  # Keep encoder_layer_6\n",
    "    # vit_b_16.encoder.layers[7],  # Keep encoder_layer_7\n",
    ")\n",
    "\n",
    "vit_b_16.heads = nn.Sequential(\n",
    "    nn.Linear(num_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10),  \n",
    "    nn.Softmax(dim=1)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vit_b_16.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef27644",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vit_b_16.parameters())\n",
    "print('learning rate: ', optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "model_description = \"vit grad = False, normalized = imagenet mean and std\"\n",
    "\n",
    "def show_log(folder_name, log_str):\n",
    "    print(log_str)\n",
    "    \n",
    "    log_folder = f\"logs/{folder_name}\"\n",
    "    os.makedirs(log_folder, exist_ok=True)\n",
    "    log_filename = f\"{log_folder}/log.txt\"\n",
    "\n",
    "    log_file = open(log_filename, 'a')\n",
    "    log_file.write(log_str)\n",
    "    log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3cd58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_function, optimizer, epochs=25, patience=10):\n",
    "    historic = []\n",
    "    best_accuracy = 0.0\n",
    "    best_accuracy_epoch = 1\n",
    "    best_model = None\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    log_folder = \"log_\" + datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
    "    \n",
    "    show_log(log_folder, f\"Description: {model_description}\\n\")\n",
    "    show_log(log_folder, f\"loss_function: {loss_function}\\n\")\n",
    "    show_log(log_folder, f\"optimizer: {optimizer}\\n\")\n",
    "    show_log(log_folder, f\"epochs: {epochs}\\n\")\n",
    "    show_log(log_folder, f\"patience: {patience}\\n\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        init_epoch = time.time()\n",
    "        log_str = f\"\\nEpoch: {epoch+1}/{epochs}\\n\"\n",
    "        show_log(log_folder, log_str)\n",
    "\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "\n",
    "        loss_train = 0.0\n",
    "        accuracy_train = 0.0\n",
    "\n",
    "        loss_validation = 0.0\n",
    "        accuracy_validation = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(data_loader_train):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item() * inputs.size(0)\n",
    "\n",
    "            values_max, index_values_max = torch.max(outputs.data, 1)\n",
    "            correct_predictions = index_values_max.eq(labels.data.view_as(index_values_max))\n",
    "\n",
    "            accuracy = torch.mean(correct_predictions.type(torch.FloatTensor))\n",
    "            accuracy_train += accuracy.item() * inputs.size(0)\n",
    "\n",
    "            # log_str = f\"Train - batch number {i:03d}, Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}\\n\"\n",
    "            # show_log(log_folder, log_str)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for j, (inputs, labels) in enumerate(data_loader_validation):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = loss_function(outputs, labels)\n",
    "\n",
    "                loss_validation += loss.item() * inputs.size(0)\n",
    "\n",
    "                values_max, index_values_max = torch.max(outputs.data, 1)\n",
    "                correct_predictions = index_values_max.eq(labels.data.view_as(index_values_max))\n",
    "\n",
    "                accuracy = torch.mean(correct_predictions.type(torch.FloatTensor))\n",
    "                accuracy_validation += accuracy.item() * inputs.size(0)\n",
    "\n",
    "                # log_str = \"Validation - batch number: {:03d}, Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), accuracy.item())\n",
    "                # show_log(log_folder, log_str)\n",
    "\n",
    "        loss_mean_train = loss_train / train_images_length\n",
    "        accuracy_mean_train = accuracy_train / train_images_length\n",
    "\n",
    "        loss_mean_validation = loss_validation / validation_images_length\n",
    "        accuracy_mean_validation = accuracy_validation / validation_images_length\n",
    "\n",
    "        historic.append([\n",
    "            loss_mean_train,\n",
    "            loss_mean_validation,\n",
    "            accuracy_mean_train,\n",
    "            accuracy_mean_validation\n",
    "        ])\n",
    "\n",
    "        end_epoch = time.time()\n",
    "\n",
    "        log_str = f\"Epoch: {epoch+1:03d}, Train: Loss: {loss_mean_train:.4f}, Accuracy: {accuracy_mean_train*100:.4f}%,\\n\\tValidation: Loss: {loss_mean_validation:.4f}, Accuracy: {accuracy_mean_validation*100:.4f}%, Time: {end_epoch-init_epoch:.4f}s\\n\\tBest accuracy: {best_accuracy:.4f}, Best_accuracy_epoch: {best_accuracy_epoch + 1}\\n\"\n",
    "        show_log(log_folder, log_str)\n",
    "        \n",
    "        if accuracy_mean_validation > best_accuracy:\n",
    "            best_accuracy = accuracy_mean_validation\n",
    "            best_accuracy_epoch = epoch\n",
    "            torch.save(model, f\"logs/{log_folder}/best_model.pth\")\n",
    "            best_model = model\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                log_str = f\"\\nEarly stopping triggered! No improvement in validation accuracy for {patience} epochs.\\n\"\n",
    "                show_log(log_folder, log_str)\n",
    "\n",
    "                break\n",
    "\n",
    "    log_file.close()\n",
    "    return best_model, historic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017089c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "trained_model, historic = train_model(vit_b_16, loss_function, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b2b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df090c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# historic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3951a66",
   "metadata": {},
   "source": [
    "# Evaluating the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46399a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn about Matplotlib with Kizzy: https://youtu.be/iSpi3rKdoLQ\n",
    "# Matplotlib introduction | Python Graphs | Data analysis #7\n",
    "def plot_losses(losses):\n",
    "  fig = plt.figure(figsize=(13, 5))\n",
    "  ax = fig.gca()\n",
    "  for loss_name, loss_values in losses.items():\n",
    "    ax.plot(loss_values, label=loss_name)\n",
    "  ax.legend(fontsize=\"16\")\n",
    "  ax.set_xlabel(\"Iteration\", fontsize=\"16\")\n",
    "  ax.set_ylabel(\"Loss\", fontsize=\"16\")\n",
    "  ax.set_title(\"Loss vs iterations\", fontsize=\"16\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f817b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(model, loader, n_classes):\n",
    "  confusion_matrix = torch.zeros(n_classes, n_classes, dtype=torch.int64)\n",
    "  with torch.no_grad():\n",
    "    for i, (imgs, labels) in enumerate(loader):\n",
    "      imgs = imgs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model(imgs)\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      for t, p in zip(torch.as_tensor(labels, dtype=torch.int64).view(-1),\n",
    "                      torch.as_tensor(predicted, dtype=torch.int64).view(-1)):\n",
    "        confusion_matrix[t, p] += 1\n",
    "  return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataloader, classes, verbose=True):\n",
    "  # prepare to count predictions for each class\n",
    "  correct_pred = {classname: 0 for classname in classes}\n",
    "  total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "  confusion_matrix = make_confusion_matrix(model, dataloader, len(classes))\n",
    "  if verbose:\n",
    "    total_correct = 0.0\n",
    "    total_prediction = 0.0\n",
    "    for i, classname in enumerate(classes):\n",
    "      correct_count = confusion_matrix[i][i].item()\n",
    "      class_pred = torch.sum(confusion_matrix[i]).item()\n",
    "\n",
    "      total_correct += correct_count\n",
    "      total_prediction += class_pred\n",
    "\n",
    "      accuracy = 100 * float(correct_count) / class_pred\n",
    "      print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                    accuracy))\n",
    "  print(\"Global acccuracy is {:.1f}\".format(100 * total_correct/total_prediction))\n",
    "  return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\"Train Loss\": loss_mean_train, \"Test Loss\": loss_mean_validation}\n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = evaluate_accuracy(vit_b_16, data_loader_validation, CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5356cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "plt.figure(figsize=(12, 12))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(confusion_matrix.tolist(),\n",
    "           annot=True, annot_kws={\"size\": 16}, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d198b",
   "metadata": {},
   "source": [
    "# Testing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e5350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Get the path of the image relative to the current working directory\n",
    "image_path = os.path.abspath('./images/bird1.png')\n",
    "\n",
    "# Open and display the image\n",
    "img = Image.open(image_path)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab5ec84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(img_tensor.permute(1,2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = img_tensor.unsqueeze(0).to(device)\n",
    "net.eval()\n",
    "output = net(batch)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd72993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.nn.functional.softmax(output, dim=1) * 100\n",
    "prob_dict = {}\n",
    "for i, classname in enumerate(CATEGORIES):\n",
    "  prob = logits[0][i].item()\n",
    "  print(f\"{classname} score: {prob:.2f}\")\n",
    "  prob_dict[classname] = [prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8fb3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_prob = pd.DataFrame.from_dict(prob_dict)\n",
    "df_prob.plot(kind='barh', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303567a",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd4562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/some_models/mlp_model_weights.pth')\n",
    "torch.save(net.state_dict(), './models/cifar10_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84b9237",
   "metadata": {},
   "source": [
    "# Creating the onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a23cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model weights\n",
    "model_url = './models/cifar10_model_weights.pth'\n",
    "model_to_onnx = ConvolutionalModel()\n",
    "\n",
    "# Initialize model with the pretrained weights\n",
    "device = torch.device('cuda')\n",
    "device\n",
    "model_to_onnx = ConvolutionalModel()\n",
    "model_to_onnx.load_state_dict(torch.load(model_url))\n",
    "# model_to_onnx.load_state_dict(torch.load(model_url, map_location=device))\n",
    "\n",
    "# set the model to inference mode\n",
    "model_to_onnx.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# dummy_input = torch.randn(1, 3, 32, 32)\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model_to_onnx,                 # model being run\n",
    "                  dummy_input,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"./models/cifar10_onnx_mnist.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fdf2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
